{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_layer = nn.TransformerEncoderLayer(d_model=8, nhead=8)\n",
    "# src = torch.tensor([[1,2,3,4,5,6,7,8]])\n",
    "# out = encoder_layer(src)\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = [0,1,2,3]\n",
    "\n",
    "np.pad(a,pad_width=(0,4-len(a))).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, output_size)\n",
    "        self.rnn = nn.GRU(output_size, output_size, batch_first=True)\n",
    "        \n",
    "        \n",
    "        #self.rnn = nn.LSTM(output_size, output_size, num_layers=num_layers,bidirectional=True)\n",
    "        \n",
    "        \n",
    "        #self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        #embedded = self.dropout(self.embedding(input))\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        return hidden\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3928, -0.1704]])\n",
      "tensor([[ 0.3928, -0.1704]])\n",
      "tensor([[ 0.2258, -0.4056]])\n",
      "tensor([[ 0.3825, -0.1454]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "enc = EncoderRNN(19,2)\n",
    "print(enc(torch.tensor([1,2,3,4])).detach())\n",
    "print(enc(torch.tensor([1,2,3,4])).detach())\n",
    "print(enc(torch.tensor([1,2,3,18,9])).detach())\n",
    "print(enc(torch.tensor([1,3,2,3])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W_q = nn.Linear(input_size, hidden_size)\n",
    "        self.W_k = nn.Linear(input_size, hidden_size)\n",
    "        self.W_v = nn.Linear(input_size, hidden_size)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_size]))\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "\n",
    "        Q = Q.unsqueeze(1)  # Add an extra dimension\n",
    "        K = K.transpose(1, 2)  # Transpose to get the correct shape\n",
    "        attention_scores = torch.matmul(Q, K) / self.scale\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        attended_values = torch.matmul(attention_weights, V)\n",
    "\n",
    "        return attended_values, attention_weights\n",
    "\n",
    "class TextEncoderWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1,num_heads=8):\n",
    "        super(TextEncoderWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        #self.attention = Attention(hidden_size * 2, hidden_size)\n",
    "        self.attention = nn.MultiheadAttention(hidden_size,num_heads)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        embedded = self.embedding(input_sequence)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        print(f'{output=}')\n",
    "        print(f'{hidden=}')\n",
    "\n",
    "        # Use the last hidden states from both directions\n",
    "        hidden_states = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "\n",
    "        # Apply attention\n",
    "        attended_values, attention_weights = self.attention(hidden_states, output, output)\n",
    "\n",
    "        return attended_values, attention_weights\n",
    "\n",
    "# Example usage:\n",
    "vocab_size = 10  # replace with your vocabulary size\n",
    "embed_size = 32   # replace with your desired embedding size\n",
    "hidden_size = 8  # replace with your desired hidden size\n",
    "\n",
    "model = TextEncoderWithAttention(vocab_size, embed_size, hidden_size)\n",
    "#input_sequence = torch.randint(0, vocab_size, (32, 20))  # replace with your input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output=tensor([[[ 0.0368, -0.3513, -0.0332, -0.2456,  0.0197, -0.0442,  0.0710,\n",
      "           0.1692,  0.1750, -0.3338,  0.0923,  0.1968, -0.0456, -0.5608,\n",
      "          -0.2093,  0.2147],\n",
      "         [ 0.0509, -0.0438, -0.0054, -0.0192,  0.0094, -0.1162,  0.0343,\n",
      "          -0.0616,  0.1316, -0.2480,  0.1062,  0.4024, -0.3118, -0.4493,\n",
      "          -0.0595,  0.0845],\n",
      "         [-0.3997,  0.0142,  0.0871,  0.0187, -0.3732, -0.1396,  0.0146,\n",
      "           0.0992,  0.0075,  0.1440,  0.0429,  0.1881,  0.0247, -0.1914,\n",
      "          -0.4735,  0.4208]]], grad_fn=<TransposeBackward0>)\n",
      "hidden=tensor([[[-0.3997,  0.0142,  0.0871,  0.0187, -0.3732, -0.1396,  0.0146,\n",
      "           0.0992]],\n",
      "\n",
      "        [[ 0.1750, -0.3338,  0.0923,  0.1968, -0.0456, -0.5608, -0.2093,\n",
      "           0.2147]]], grad_fn=<StackBackward0>)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "For unbatched (2-D) `query`, expected `key` and `value` to be 2-D but found 3-D and 3-D tensors respectively",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/aleksei/MyProject/python/ml/test/test.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/aleksei/MyProject/python/ml/test/test.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(model(torch\u001b[39m.\u001b[39;49mtensor([[\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m3\u001b[39;49m]])))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aleksei/MyProject/python/ml/test/test.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# print(model(torch.tensor([[1,3,2]]))[0].detach().view(-1))\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aleksei/MyProject/python/ml/test/test.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# print(model(torch.tensor([[1,3,2]]))[0].detach().view(-1))\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/ml/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/aleksei/MyProject/python/ml/test/test.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aleksei/MyProject/python/ml/test/test.ipynb#W6sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((hidden[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, :, :], hidden[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :, :]), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aleksei/MyProject/python/ml/test/test.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Apply attention\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/aleksei/MyProject/python/ml/test/test.ipynb#W6sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m attended_values, attention_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(hidden_states, output, output)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aleksei/MyProject/python/ml/test/test.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mreturn\u001b[39;00m attended_values, attention_weights\n",
      "File \u001b[0;32m~/.virtualenvs/ml/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.virtualenvs/ml/lib/python3.8/site-packages/torch/nn/modules/activation.py:1167\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1157\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1158\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1165\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1166\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1168\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1169\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1170\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1172\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1173\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1174\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[1;32m   1175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1176\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/.virtualenvs/ml/lib/python3.8/site-packages/torch/nn/functional.py:5005\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   4975\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tens_ops):\n\u001b[1;32m   4976\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   4977\u001b[0m         multi_head_attention_forward,\n\u001b[1;32m   4978\u001b[0m         tens_ops,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5002\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   5003\u001b[0m     )\n\u001b[0;32m-> 5005\u001b[0m is_batched \u001b[39m=\u001b[39m _mha_shape_check(query, key, value, key_padding_mask, attn_mask, num_heads)\n\u001b[1;32m   5007\u001b[0m \u001b[39m# For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input\u001b[39;00m\n\u001b[1;32m   5008\u001b[0m \u001b[39m# is batched, run the computation and before returning squeeze the\u001b[39;00m\n\u001b[1;32m   5009\u001b[0m \u001b[39m# batch dimension so that the output doesn't carry this temporary batch dimension.\u001b[39;00m\n\u001b[1;32m   5010\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched:\n\u001b[1;32m   5011\u001b[0m     \u001b[39m# unsqueeze if the input is unbatched\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/ml/lib/python3.8/site-packages/torch/nn/functional.py:4863\u001b[0m, in \u001b[0;36m_mha_shape_check\u001b[0;34m(query, key, value, key_padding_mask, attn_mask, num_heads)\u001b[0m\n\u001b[1;32m   4860\u001b[0m \u001b[39melif\u001b[39;00m query\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   4861\u001b[0m     \u001b[39m# Unbatched Inputs\u001b[39;00m\n\u001b[1;32m   4862\u001b[0m     is_batched \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 4863\u001b[0m     \u001b[39massert\u001b[39;00m key\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m, \\\n\u001b[1;32m   4864\u001b[0m         (\u001b[39m\"\u001b[39m\u001b[39mFor unbatched (2-D) `query`, expected `key` and `value` to be 2-D\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4865\u001b[0m          \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but found \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D and \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D tensors respectively\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   4867\u001b[0m     \u001b[39mif\u001b[39;00m key_padding_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4868\u001b[0m         \u001b[39massert\u001b[39;00m key_padding_mask\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m, \\\n\u001b[1;32m   4869\u001b[0m             (\u001b[39m\"\u001b[39m\u001b[39mFor unbatched (2-D) `query`, expected `key_padding_mask` to be `None` or 1-D\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4870\u001b[0m              \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but found \u001b[39m\u001b[39m{\u001b[39;00mkey_padding_mask\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D tensor instead\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: For unbatched (2-D) `query`, expected `key` and `value` to be 2-D but found 3-D and 3-D tensors respectively"
     ]
    }
   ],
   "source": [
    "print(model(torch.tensor([[1,2,3]])))\n",
    "# print(model(torch.tensor([[1,3,2]]))[0].detach().view(-1))\n",
    "# print(model(torch.tensor([[1,3,2]]))[0].detach().view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0744, -0.1406, -0.1321, -0.5291, -0.2384]]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextEncoderWithMultiheadAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_heads, num_layers=1):\n",
    "        super(TextEncoderWithMultiheadAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_size, num_heads)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        embedded = self.embedding(input_sequence)\n",
    "        \n",
    "        # Permute dimensions for MultiheadAttention\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "\n",
    "        # Apply Multihead Attention\n",
    "        attention_output, _ = self.multihead_attention(embedded, embedded, embedded)\n",
    "        \n",
    "        # Reshape and permute back\n",
    "        attention_output = attention_output.permute(1, 0, 2)\n",
    "\n",
    "        # LSTM encoding\n",
    "        output, (hidden, cell) = self.lstm(attention_output)\n",
    "        #print(f'{hidden=}')\n",
    "\n",
    "        # Use the last hidden states from both directions\n",
    "        #hidden_states = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "# Example usage:\n",
    "vocab_size = 10000  # replace with your vocabulary size\n",
    "embed_size = 256   # replace with your desired embedding size\n",
    "hidden_size = 5  # replace with your desired hidden size\n",
    "num_heads = 8      # replace with your desired number of heads\n",
    "\n",
    "model = TextEncoderWithMultiheadAttention(vocab_size, embed_size, hidden_size, num_heads)\n",
    "#input_sequence = torch.randint(0, vocab_size, (32, 20))  # replace with your input data\n",
    "input_sequence = torch.tensor([[1,2,3]])\n",
    "output = model(input_sequence)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
